{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c845ec98",
   "metadata": {
    "papermill": {
     "duration": 0.014724,
     "end_time": "2024-01-30T00:13:56.963222",
     "exception": false,
     "start_time": "2024-01-30T00:13:56.948498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a slightly improved version of \"[Enefit PEBOP: EDA (Plotly) and Modelling](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling?scriptVersionId=158742203)\" notebook, namely 3-rd version that scores best on public LB. List of changes:\n",
    "- Trained models are now loaded from an external [dataset](https://www.kaggle.com/datasets/kononenko/v2-enefit-pebop-eda-plotly-and-modelling/data), instead of a notebook. This fixes issues in the case a new notebook version is created;\n",
    "- Ensemble weights are tuned to maximize LB score.\n",
    "\n",
    "If you find this notebook useful, please appreciate the original work by @siddhvr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb888c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:13:56.994022Z",
     "iopub.status.busy": "2024-01-30T00:13:56.993573Z",
     "iopub.status.idle": "2024-01-30T00:14:14.810615Z",
     "shell.execute_reply": "2024-01-30T00:14:14.809528Z"
    },
    "papermill": {
     "duration": 17.835567,
     "end_time": "2024-01-30T00:14:14.813405",
     "exception": false,
     "start_time": "2024-01-30T00:13:56.977838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/deeptables-dependecies\r\n",
      "Processing /kaggle/input/deeptables-dependecies/deeptables-0.2.5-py3-none-any.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (21.3)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.11.4)\r\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.2.2)\r\n",
      "Requirement already satisfied: lightgbm>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (3.3.2)\r\n",
      "Requirement already satisfied: category-encoders>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.6.3)\r\n",
      "Processing /kaggle/input/deeptables-dependecies/hypernets-0.3.1-py3-none-any.whl (from deeptables==0.2.5)\r\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (3.9.0)\r\n",
      "Requirement already satisfied: eli5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (0.13.0)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.14.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.5.3)\r\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2023.12.2)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (8.14.0)\r\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.0)\r\n",
      "Processing /kaggle/input/deeptables-dependecies/XlsxWriter-3.1.9-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.0.1)\r\n",
      "Processing /kaggle/input/deeptables-dependecies/paramiko-3.4.0-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2.31.0)\r\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.3.3)\r\n",
      "Requirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (3.8.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (4.66.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (1.3.2)\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm>=2.2.0->deeptables==0.2.5) (0.41.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2023.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->deeptables==0.2.5) (3.2.0)\r\n",
      "Requirement already satisfied: attrs>17.1.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (23.1.0)\r\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (3.1.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (1.16.0)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.20.1)\r\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->deeptables==0.2.5) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->eli5->deeptables==0.2.5) (2.1.3)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.19.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (3.0.39)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.16.1)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.6.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.8.0)\r\n",
      "Processing /kaggle/input/deeptables-dependecies/bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (41.0.3)\r\n",
      "Processing /kaggle/input/deeptables-dependecies/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2023.11.17)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (1.15.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (1.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.2.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.2)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (2.21)\r\n",
      "Installing collected packages: XlsxWriter, bcrypt, pynacl, paramiko, hypernets, deeptables\r\n",
      "Successfully installed XlsxWriter-3.1.9 bcrypt-4.1.2 deeptables-0.2.5 hypernets-0.3.1 paramiko-3.4.0 pynacl-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/deeptables-dependecies deeptables==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f9b512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:14.849430Z",
     "iopub.status.busy": "2024-01-30T00:14:14.848597Z",
     "iopub.status.idle": "2024-01-30T00:14:36.341175Z",
     "shell.execute_reply": "2024-01-30T00:14:36.340141Z"
    },
    "papermill": {
     "duration": 21.513738,
     "end_time": "2024-01-30T00:14:36.343985",
     "exception": false,
     "start_time": "2024-01-30T00:14:14.830247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import ctypes\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf, deeptables as dt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.python.keras import backend as K\n",
    "from deeptables.models import DeepTable, ModelConfig\n",
    "from deeptables.models import deepnets\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import lightgbm as lgb\n",
    "from joblib import load\n",
    "\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98dc5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:36.379937Z",
     "iopub.status.busy": "2024-01-30T00:14:36.379488Z",
     "iopub.status.idle": "2024-01-30T00:14:36.855617Z",
     "shell.execute_reply": "2024-01-30T00:14:36.854496Z"
    },
    "papermill": {
     "duration": 0.496978,
     "end_time": "2024-01-30T00:14:36.858369",
     "exception": false,
     "start_time": "2024-01-30T00:14:36.361391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stacking average model\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "from catboost import Pool\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0e1b50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:36.893603Z",
     "iopub.status.busy": "2024-01-30T00:14:36.893169Z",
     "iopub.status.idle": "2024-01-30T00:14:37.184295Z",
     "shell.execute_reply": "2024-01-30T00:14:37.183327Z"
    },
    "papermill": {
     "duration": 0.311478,
     "end_time": "2024-01-30T00:14:37.186662",
     "exception": false,
     "start_time": "2024-01-30T00:14:36.875184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d919290",
   "metadata": {
    "papermill": {
     "duration": 0.016252,
     "end_time": "2024-01-30T00:14:37.219743",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.203491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> <h4> This is a bonus version of this Notebook, since I have exhausted my entire GPU quota for the week ðŸ™ƒ. \n",
    "Here, I have tried to ensemble the predictions of two trained models. But the catch is that both were trained on datasets having different number of features. I have tried to explore the implications of this difference on the end predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a48c8",
   "metadata": {
    "papermill": {
     "duration": 0.016652,
     "end_time": "2024-01-30T00:14:37.253284",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.236632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> <h4> I will be using a trained model which I published earlier as a dataset. It had an independent MAE of 65.51 on the public test set (LeaderBoard) and the other model will be the one which we trained on Version 1[(see here)](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling) of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aeefad",
   "metadata": {
    "papermill": {
     "duration": 0.016828,
     "end_time": "2024-01-30T00:14:37.287209",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.270381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1 : Create a class to access and process all the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb421b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:37.322802Z",
     "iopub.status.busy": "2024-01-30T00:14:37.322371Z",
     "iopub.status.idle": "2024-01-30T00:14:37.351554Z",
     "shell.execute_reply": "2024-01-30T00:14:37.350131Z"
    },
    "papermill": {
     "duration": 0.050426,
     "end_time": "2024-01-30T00:14:37.354433",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.304007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "    data_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"row_id\",\n",
    "    ]\n",
    "    client_cols = [\n",
    "        \"product_type\",\n",
    "        \"county\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_cols = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"hours_ahead\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"forecast_datetime\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "    historical_weather_cols = [\n",
    "        \"datetime\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(self.root, \"train.csv\"),\n",
    "            columns=self.data_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(self.root, \"client.csv\"),\n",
    "            columns=self.client_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_gas_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"gas_prices.csv\"),\n",
    "            columns=self.gas_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"electricity_prices.csv\"),\n",
    "            columns=self.electricity_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_data = self.df_data.filter(\n",
    "            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n",
    "        )\n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_with_new_data(\n",
    "        self,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "    ):\n",
    "        df_new_client = pl.from_pandas(\n",
    "            df_new_client[self.client_cols], schema_overrides=self.schema_client\n",
    "        )\n",
    "        df_new_gas_prices = pl.from_pandas(\n",
    "            df_new_gas_prices[self.gas_prices_cols],\n",
    "            schema_overrides=self.schema_gas_prices,\n",
    "        )\n",
    "        df_new_electricity_prices = pl.from_pandas(\n",
    "            df_new_electricity_prices[self.electricity_prices_cols],\n",
    "            schema_overrides=self.schema_electricity_prices,\n",
    "        )\n",
    "        df_new_forecast_weather = pl.from_pandas(\n",
    "            df_new_forecast_weather[self.forecast_weather_cols],\n",
    "            schema_overrides=self.schema_forecast_weather,\n",
    "        )\n",
    "        df_new_historical_weather = pl.from_pandas(\n",
    "            df_new_historical_weather[self.historical_weather_cols],\n",
    "            schema_overrides=self.schema_historical_weather,\n",
    "        )\n",
    "        df_new_target = pl.from_pandas(\n",
    "            df_new_target[self.target_cols], schema_overrides=self.schema_target\n",
    "        )\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n",
    "            [\"date\", \"county\", \"is_business\", \"product_type\"]\n",
    "        )\n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n",
    "            [\"forecast_date\"]\n",
    "        )\n",
    "        self.df_electricity_prices = pl.concat(\n",
    "            [self.df_electricity_prices, df_new_electricity_prices]\n",
    "        ).unique([\"forecast_date\"])\n",
    "        self.df_forecast_weather = pl.concat(\n",
    "            [self.df_forecast_weather, df_new_forecast_weather]\n",
    "        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat(\n",
    "            [self.df_historical_weather, df_new_historical_weather]\n",
    "        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df6343",
   "metadata": {
    "papermill": {
     "duration": 0.016422,
     "end_time": "2024-01-30T00:14:37.387938",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.371516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 2: Create Feature Enegineering Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c2a25",
   "metadata": {
    "papermill": {
     "duration": 0.017086,
     "end_time": "2024-01-30T00:14:37.421704",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.404618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> <h4> For ease of understanding and clarity, I will be using two different feature engineering classes for both models. However, we can also merge them and make a single class since, most of the functions are the same with minor changes elsewhere. But remember to have two different <i>'generate_features'</i> functions for both of these in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adda482f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:37.457723Z",
     "iopub.status.busy": "2024-01-30T00:14:37.457247Z",
     "iopub.status.idle": "2024-01-30T00:14:37.511348Z",
     "shell.execute_reply": "2024-01-30T00:14:37.510370Z"
    },
    "papermill": {
     "duration": 0.075352,
     "end_time": "2024-01-30T00:14:37.513904",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.438552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _client_features(self, df_features):\n",
    "        df_client = self.data.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "#             .drop(\"hours_ahead\")\n",
    "            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n",
    "            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n",
    "\n",
    "        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _target_features(self, df_features):\n",
    "        df_target = self.data.df_target\n",
    "\n",
    "        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n",
    "\n",
    "        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n",
    "        \n",
    "        hours_list=[i*24 for i in range(2,15)]\n",
    "\n",
    "        for hours_lag in hours_list:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "            )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "        \n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    # added some new features here\n",
    "    def _additional_features(self,df):\n",
    "        for col in [\n",
    "                    'temperature', \n",
    "                    'dewpoint', \n",
    "                    '10_metre_u_wind_component', \n",
    "                    '10_metre_v_wind_component', \n",
    "            ]:\n",
    "            for window in [1]:\n",
    "                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n",
    "        return df\n",
    "    \n",
    "    def _log_outliers(self,df):\n",
    "        l1=['installed_capacity', 'target_mean', 'target_std']\n",
    "        for i in l1:\n",
    "            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def generate_features(self, df_prediction_items,isTrain):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._general_features,self._client_features,self._forecast_weather_features,\n",
    "            self._historical_weather_features,self._target_features,self._holidays_features,\n",
    "            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        df_features = self._additional_features(df_features)\n",
    "\n",
    "        return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3833aebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:37.549825Z",
     "iopub.status.busy": "2024-01-30T00:14:37.549168Z",
     "iopub.status.idle": "2024-01-30T00:14:37.599090Z",
     "shell.execute_reply": "2024-01-30T00:14:37.597949Z"
    },
    "papermill": {
     "duration": 0.07126,
     "end_time": "2024-01-30T00:14:37.601955",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.530695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeaturesGenerator:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab96ef",
   "metadata": {
    "papermill": {
     "duration": 0.016265,
     "end_time": "2024-01-30T00:14:37.634893",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.618628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Featureweather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286510ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:37.670035Z",
     "iopub.status.busy": "2024-01-30T00:14:37.669617Z",
     "iopub.status.idle": "2024-01-30T00:14:37.733444Z",
     "shell.execute_reply": "2024-01-30T00:14:37.732169Z"
    },
    "papermill": {
     "duration": 0.085047,
     "end_time": "2024-01-30T00:14:37.736365",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.651318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureWeather:\n",
    "    def __init__(self, data_weather):\n",
    "        self.data_storage = data_weather\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "        \n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "        for hours_lag in [0, 2 * 24, 3 * 24, 7 * 24]:    \n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )  \n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features      \n",
    "    \n",
    "    \n",
    "    def _add_weather_difference_features(self, df_features):\n",
    "        weather_diff_cols = ['direct_solar_radiation', 'surface_solar_radiation_downwards','temperature','dewpoint']\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24]:\n",
    "            for col in weather_diff_cols:\n",
    "                df_features = df_features.with_columns(\n",
    "                   (pl.col(f'{col}_forecast_local_0h') -  pl.col(f'{col}_forecast_local_{hours_lag}h')).alias(f'{col}_forecast_l_diff_{hours_lag}h'),\n",
    "                )\n",
    "                \n",
    "        for forecast_prefix, lag_nominator, lag_denomonator in [\n",
    "            ('direct_solar_radiation', 24 * 2, 24 * 3),\n",
    "            ('direct_solar_radiation', 24 * 2, 24 * 7),\n",
    "            ('direct_solar_radiation', 24 * 3, 24 * 7),\n",
    "            ('surface_solar_radiation_downwards', 24 * 2, 24 * 3),\n",
    "            ('surface_solar_radiation_downwards', 24 * 2, 24 * 7),\n",
    "            ('surface_solar_radiation_downwards', 24 * 3, 24 * 7),\n",
    "            ('temperature', 24 * 2, 24 * 3),\n",
    "            ('temperature', 24 * 2, 24 * 7),\n",
    "            ('temperature', 24 * 3, 24 * 7),\n",
    "            ('dewpoint', 24 * 2, 24 * 3),\n",
    "            ('dewpoint', 24 * 2, 24 * 7),\n",
    "            ('dewpoint', 24 * 3, 24 * 7),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{forecast_prefix}_forecast_local_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{forecast_prefix}_forecast_local_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{forecast_prefix}_forecast_l_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )  \n",
    "            \n",
    "        return df_features  \n",
    "            \n",
    "            \n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "        ]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "               (pl.col('target_48h')/pl.col('installed_capacity')).alias('target_48h_installed')\n",
    "        )\n",
    "        df_features = df_features.with_columns(\n",
    "               (pl.col('target_72h')/pl.col('installed_capacity')).alias('target_72h_installed')\n",
    "        )\n",
    "        df_features = df_features.with_columns(\n",
    "               (pl.col('target_96h')/pl.col('installed_capacity')).alias('target_96h_installed')\n",
    "        )\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "               ((pl.col('eic_count') - pl.col('eic_count').mean())/ pl.col('eic_count').mean() * 100).alias('normalized_eic_count')\n",
    "        ) \n",
    "        df_features = df_features.with_columns(\n",
    "               ((pl.col('installed_capacity') - pl.col('installed_capacity').mean())/ pl.col('installed_capacity').mean() * 100).alias('normalized_installed_capacity')\n",
    "        )    \n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_weather_difference_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e717a8b",
   "metadata": {
    "papermill": {
     "duration": 0.016985,
     "end_time": "2024-01-30T00:14:37.771439",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.754454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f24220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:37.806447Z",
     "iopub.status.busy": "2024-01-30T00:14:37.806045Z",
     "iopub.status.idle": "2024-01-30T00:14:44.293848Z",
     "shell.execute_reply": "2024-01-30T00:14:44.292687Z"
    },
    "papermill": {
     "duration": 6.508556,
     "end_time": "2024-01-30T00:14:44.296604",
     "exception": false,
     "start_time": "2024-01-30T00:14:37.788048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)\n",
    "feat_gen = FeatureEngineer(data=data_storage)\n",
    "feat_weather = FeatureWeather(data_weather=data_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0a4cb",
   "metadata": {
    "papermill": {
     "duration": 0.016829,
     "end_time": "2024-01-30T00:14:44.330953",
     "exception": false,
     "start_time": "2024-01-30T00:14:44.314124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464eeff",
   "metadata": {
    "papermill": {
     "duration": 0.016333,
     "end_time": "2024-01-30T00:14:44.364373",
     "exception": false,
     "start_time": "2024-01-30T00:14:44.348040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> <h5>Although there is no requirement as such to create the training datasets, (since we are not doing any model training here) I have created them only to show the difference in the features between the two datasets we are using here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb55574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:14:44.463180Z",
     "iopub.status.busy": "2024-01-30T00:14:44.462753Z",
     "iopub.status.idle": "2024-01-30T00:15:16.083348Z",
     "shell.execute_reply": "2024-01-30T00:15:16.081515Z"
    },
    "papermill": {
     "duration": 31.642614,
     "end_time": "2024-01-30T00:15:16.086381",
     "exception": false,
     "start_time": "2024-01-30T00:14:44.443767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_weather = feat_weather.generate_features(data_storage.df_data)\n",
    "df_train_weather = df_train_weather[df_train_weather['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00abadea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:15:16.123722Z",
     "iopub.status.busy": "2024-01-30T00:15:16.122446Z",
     "iopub.status.idle": "2024-01-30T00:16:02.659644Z",
     "shell.execute_reply": "2024-01-30T00:16:02.658271Z"
    },
    "papermill": {
     "duration": 46.561552,
     "end_time": "2024-01-30T00:16:02.665298",
     "exception": false,
     "start_time": "2024-01-30T00:15:16.103746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc8e32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:02.705807Z",
     "iopub.status.busy": "2024-01-30T00:16:02.705382Z",
     "iopub.status.idle": "2024-01-30T00:16:33.721917Z",
     "shell.execute_reply": "2024-01-30T00:16:33.720588Z"
    },
    "papermill": {
     "duration": 31.039163,
     "end_time": "2024-01-30T00:16:33.725185",
     "exception": false,
     "start_time": "2024-01-30T00:16:02.686022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = feat_gen.generate_features(data_storage.df_data,True)\n",
    "df_train = df_train[df_train['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea28dcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:33.761885Z",
     "iopub.status.busy": "2024-01-30T00:16:33.761118Z",
     "iopub.status.idle": "2024-01-30T00:16:33.771458Z",
     "shell.execute_reply": "2024-01-30T00:16:33.770191Z"
    },
    "papermill": {
     "duration": 0.031249,
     "end_time": "2024-01-30T00:16:33.773822",
     "exception": false,
     "start_time": "2024-01-30T00:16:33.742573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1651902, 166)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b51cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:33.812641Z",
     "iopub.status.busy": "2024-01-30T00:16:33.811918Z",
     "iopub.status.idle": "2024-01-30T00:16:33.818291Z",
     "shell.execute_reply": "2024-01-30T00:16:33.817368Z"
    },
    "papermill": {
     "duration": 0.0291,
     "end_time": "2024-01-30T00:16:33.820816",
     "exception": false,
     "start_time": "2024-01-30T00:16:33.791716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1651902, 176)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a46ca8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:33.858869Z",
     "iopub.status.busy": "2024-01-30T00:16:33.858182Z",
     "iopub.status.idle": "2024-01-30T00:16:33.865447Z",
     "shell.execute_reply": "2024-01-30T00:16:33.864327Z"
    },
    "papermill": {
     "duration": 0.028852,
     "end_time": "2024-01-30T00:16:33.867652",
     "exception": false,
     "start_time": "2024-01-30T00:16:33.838800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1651902, 219)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e524ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:33.905183Z",
     "iopub.status.busy": "2024-01-30T00:16:33.904365Z",
     "iopub.status.idle": "2024-01-30T00:16:34.205486Z",
     "shell.execute_reply": "2024-01-30T00:16:34.204128Z"
    },
    "papermill": {
     "duration": 0.322789,
     "end_time": "2024-01-30T00:16:34.208227",
     "exception": false,
     "start_time": "2024-01-30T00:16:33.885438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e12c0890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:34.246157Z",
     "iopub.status.busy": "2024-01-30T00:16:34.245733Z",
     "iopub.status.idle": "2024-01-30T00:16:34.252889Z",
     "shell.execute_reply": "2024-01-30T00:16:34.251376Z"
    },
    "papermill": {
     "duration": 0.029855,
     "end_time": "2024-01-30T00:16:34.255787",
     "exception": false,
     "start_time": "2024-01-30T00:16:34.225932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "hours_ahead\n",
      "hours_ahead_forecast_local_0h\n",
      "hours_ahead_forecast_168h\n",
      "hours_ahead_forecast_local_168h\n",
      "literal\n",
      "temperature_diff_1\n",
      "dewpoint_diff_1\n",
      "10_metre_u_wind_component_diff_1\n",
      "10_metre_v_wind_component_diff_1\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns:\n",
    "    if (col not in df_train_features.columns):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1eecc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:34.295325Z",
     "iopub.status.busy": "2024-01-30T00:16:34.294929Z",
     "iopub.status.idle": "2024-01-30T00:16:34.300186Z",
     "shell.execute_reply": "2024-01-30T00:16:34.299051Z"
    },
    "papermill": {
     "duration": 0.027987,
     "end_time": "2024-01-30T00:16:34.302503",
     "exception": false,
     "start_time": "2024-01-30T00:16:34.274516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if('date' in df_train_features.columns):\n",
    "    df_train_features.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c4fe8",
   "metadata": {
    "papermill": {
     "duration": 0.017838,
     "end_time": "2024-01-30T00:16:34.338605",
     "exception": false,
     "start_time": "2024-01-30T00:16:34.320767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DeepTables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7189ccb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:34.377086Z",
     "iopub.status.busy": "2024-01-30T00:16:34.376696Z",
     "iopub.status.idle": "2024-01-30T00:16:34.716716Z",
     "shell.execute_reply": "2024-01-30T00:16:34.715483Z"
    },
    "papermill": {
     "duration": 0.362585,
     "end_time": "2024-01-30T00:16:34.719320",
     "exception": false,
     "start_time": "2024-01-30T00:16:34.356735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 1e-07 to 0.001 to 1e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_START = 1e-7\n",
    "LR_MAX = 1e-3\n",
    "LR_MIN = 1e-7\n",
    "LR_RAMPUP_EPOCHS = 2\n",
    "LR_SUSTAIN_EPOCHS = 2\n",
    "EPOCHS = 10\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "        cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n",
    "        \n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "lr_y = [lrfn(x) for x in rng]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rng, lr_y, '-o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('LR')\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n",
    "      format(lr_y[0], max(lr_y), lr_y[-1]))\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57e0d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T00:16:34.759341Z",
     "iopub.status.busy": "2024-01-30T00:16:34.758933Z",
     "iopub.status.idle": "2024-01-30T02:38:46.252457Z",
     "shell.execute_reply": "2024-01-30T02:38:46.250758Z"
    },
    "papermill": {
     "duration": 8531.561305,
     "end_time": "2024-01-30T02:38:46.299596",
     "exception": false,
     "start_time": "2024-01-30T00:16:34.738291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn = True\n",
      "lgb = True\n",
      "\n",
      " nn model consumption training. \n",
      "\n",
      "01-30 00:16:36 I deeptables.m.deeptable.py 338 - X.Shape=(825951, 218), y.Shape=(825951,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e40b3ff0280>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 00:16:36 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 00:16:38 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 00:16:40 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 00:16:43 I deeptables.m.preprocessor.py 336 - Preparing features taken 2.836514949798584s\n",
      "01-30 00:16:43 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 00:16:52 I deeptables.m.preprocessor.py 383 - Imputation taken 8.935818195343018s\n",
      "01-30 00:16:52 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 00:16:52 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.2370901107788086s\n",
      "01-30 00:16:52 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 00:16:52 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 00:17:04 I deeptables.m.preprocessor.py 404 - Discretization taken 12.422903776168823s\n",
      "01-30 00:17:09 I deeptables.m.preprocessor.py 196 - fit_transform taken 30.38438057899475s\n",
      "01-30 00:17:09 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 00:17:09 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_m, patience:1, mode:min\n",
      "01-30 00:17:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 00:17:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 00:17:19 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 00:17:21 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (217)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [18, 4, 6, 71, 4, 4, 4, 4, 3, 6, 6, 4, 4, 7, 8, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 17, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 23, 23, 23, 23, 23, 23, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 26, 15, 26, 15, 26, 14, 26, 14, 30, 32, 32, 32, 32, 32, 27, 26, 15, 14, 32, 32, 32, 7, 8]\n",
      "output_dims: [8, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1488)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1488), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 00:17:21 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "1532/1532 - 142s - loss: 76.9028 - mean_absolute_error: 76.9028 - val_loss: 52.6443 - val_mean_absolute_error: 52.6443 - 142s/epoch - 93ms/step\n",
      "Epoch 2/10\n",
      "1532/1532 - 102s - loss: 59.1268 - mean_absolute_error: 59.1268 - val_loss: 47.6649 - val_mean_absolute_error: 47.6649 - 102s/epoch - 67ms/step\n",
      "Epoch 3/10\n",
      "1532/1532 - 100s - loss: 55.4141 - mean_absolute_error: 55.4141 - val_loss: 45.4247 - val_mean_absolute_error: 45.4247 - 100s/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "1532/1532 - 100s - loss: 52.7127 - mean_absolute_error: 52.7127 - val_loss: 43.5910 - val_mean_absolute_error: 43.5910 - 100s/epoch - 65ms/step\n",
      "Epoch 5/10\n",
      "1532/1532 - 108s - loss: 50.8130 - mean_absolute_error: 50.8130 - val_loss: 42.3522 - val_mean_absolute_error: 42.3522 - 108s/epoch - 70ms/step\n",
      "Epoch 6/10\n",
      "1532/1532 - 104s - loss: 49.1743 - mean_absolute_error: 49.1743 - val_loss: 40.7644 - val_mean_absolute_error: 40.7644 - 104s/epoch - 68ms/step\n",
      "Epoch 7/10\n",
      "1532/1532 - 102s - loss: 47.6593 - mean_absolute_error: 47.6593 - val_loss: 40.1741 - val_mean_absolute_error: 40.1741 - 102s/epoch - 66ms/step\n",
      "Epoch 8/10\n",
      "1532/1532 - 103s - loss: 46.3210 - mean_absolute_error: 46.3210 - val_loss: 39.6827 - val_mean_absolute_error: 39.6827 - 103s/epoch - 67ms/step\n",
      "Epoch 9/10\n",
      "1532/1532 - 100s - loss: 45.3582 - mean_absolute_error: 45.3582 - val_loss: 38.5475 - val_mean_absolute_error: 38.5475 - 100s/epoch - 65ms/step\n",
      "Epoch 10/10\n",
      "1532/1532 - 100s - loss: 44.3442 - mean_absolute_error: 44.3442 - val_loss: 38.2245 - val_mean_absolute_error: 38.2245 - 100s/epoch - 65ms/step\n",
      "01-30 00:35:27 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 00:35:27 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 00:35:27 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130001634_dnn_nets/dnn_nets.h5\n",
      "\n",
      " nn model production training. \n",
      "\n",
      "01-30 00:35:29 I deeptables.m.deeptable.py 338 - X.Shape=(825951, 218), y.Shape=(825951,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e40b3ff0280>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 00:35:29 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 00:35:31 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 00:35:32 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 00:35:34 I deeptables.m.preprocessor.py 336 - Preparing features taken 2.384514570236206s\n",
      "01-30 00:35:34 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 00:35:43 I deeptables.m.preprocessor.py 383 - Imputation taken 8.859158277511597s\n",
      "01-30 00:35:43 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 00:35:43 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.23823928833007812s\n",
      "01-30 00:35:43 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 00:35:43 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 00:35:54 I deeptables.m.preprocessor.py 404 - Discretization taken 10.780705213546753s\n",
      "01-30 00:35:58 I deeptables.m.preprocessor.py 196 - fit_transform taken 26.412356853485107s\n",
      "01-30 00:35:58 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 00:36:01 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 00:36:06 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 00:36:07 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 00:36:08 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (217)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [18, 4, 6, 71, 4, 4, 4, 4, 3, 6, 6, 4, 4, 7, 8, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 17, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 21, 14, 21, 14, 21, 14, 21, 14, 25, 29, 28, 28, 28, 28, 24, 24, 15, 14, 28, 28, 28, 7, 8]\n",
      "output_dims: [8, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1488)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1488), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 00:36:08 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "1532/1532 - 147s - loss: 31.2021 - mean_absolute_error: 31.2021 - val_loss: 24.2122 - val_mean_absolute_error: 24.2122 - 147s/epoch - 96ms/step\n",
      "Epoch 2/10\n",
      "1532/1532 - 100s - loss: 26.6361 - mean_absolute_error: 26.6361 - val_loss: 22.8603 - val_mean_absolute_error: 22.8603 - 100s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "1532/1532 - 99s - loss: 24.8120 - mean_absolute_error: 24.8120 - val_loss: 20.8732 - val_mean_absolute_error: 20.8732 - 99s/epoch - 64ms/step\n",
      "Epoch 4/10\n",
      "1532/1532 - 100s - loss: 23.4790 - mean_absolute_error: 23.4790 - val_loss: 20.1748 - val_mean_absolute_error: 20.1748 - 100s/epoch - 65ms/step\n",
      "Epoch 5/10\n",
      "1532/1532 - 100s - loss: 22.3626 - mean_absolute_error: 22.3626 - val_loss: 19.2579 - val_mean_absolute_error: 19.2579 - 100s/epoch - 65ms/step\n",
      "Epoch 6/10\n",
      "1532/1532 - 99s - loss: 21.5383 - mean_absolute_error: 21.5383 - val_loss: 18.6074 - val_mean_absolute_error: 18.6074 - 99s/epoch - 64ms/step\n",
      "Epoch 7/10\n",
      "1532/1532 - 101s - loss: 20.8623 - mean_absolute_error: 20.8623 - val_loss: 17.5066 - val_mean_absolute_error: 17.5066 - 101s/epoch - 66ms/step\n",
      "Epoch 8/10\n",
      "1532/1532 - 100s - loss: 20.2319 - mean_absolute_error: 20.2319 - val_loss: 17.1476 - val_mean_absolute_error: 17.1476 - 100s/epoch - 65ms/step\n",
      "Epoch 9/10\n",
      "1532/1532 - 101s - loss: 19.7259 - mean_absolute_error: 19.7259 - val_loss: 17.5095 - val_mean_absolute_error: 17.5095 - 101s/epoch - 66ms/step\n",
      "Epoch 10/10\n",
      "1532/1532 - 99s - loss: 19.2488 - mean_absolute_error: 19.2488 - val_loss: 16.6426 - val_mean_absolute_error: 16.6426 - 99s/epoch - 65ms/step\n",
      "01-30 00:53:35 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 00:53:35 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 00:53:36 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130003528_dnn_nets/dnn_nets.h5\n",
      "\n",
      " lgb model consumption training.\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.5\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "\n",
      " lgb model production training. \n",
      "\n",
      "CPU times: user 7h 8min 14s, sys: 12min 7s, total: 7h 20min 21s\n",
      "Wall time: 2h 22min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class CFG:\n",
    "    nn = True\n",
    "    lgb = True\n",
    "    ens_weights = {'nn': 0.5, 'lgb': 0.5}\n",
    "    epochs = 10\n",
    "    batch_size = 512\n",
    "    valid_size = 5e-2\n",
    "    LR_Scheduler = []  # [LR]\n",
    "    optimizer = AdamW(learning_rate=1e-3, weight_decay=9e-7)\n",
    "     \n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.conf = ModelConfig(auto_imputation=True,\n",
    "                                auto_discrete=True,\n",
    "                                auto_discard_unique=True,\n",
    "                                categorical_columns='auto',\n",
    "                                fixed_embedding_dim=False,\n",
    "                                embeddings_output_dim=4,\n",
    "                                embedding_dropout=0.3,\n",
    "                                nets=['dnn_nets'],\n",
    "                                dnn_params={\n",
    "                                    'hidden_units': ((512, 0.3, True),\n",
    "                                                     (256, 0.3, True)),\n",
    "                                    'dnn_activation': 'relu',\n",
    "                                },\n",
    "                                stacking_op='add',\n",
    "                                output_use_bias=False,\n",
    "                                optimizer=CFG.optimizer,\n",
    "                                task='regression',\n",
    "                                loss='MeanAbsoluteError',\n",
    "                                metrics='MeanAbsoluteError',\n",
    "                                earlystopping_patience=1,\n",
    "                                )\n",
    "        \n",
    "        self.lgb_params_production = {\n",
    "            \"n_estimators\": 2500,\n",
    "            \"learning_rate\": 0.052587652,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            \"lambda_l1\": 3.5,\n",
    "            \"lambda_l2\": 1.5,\n",
    "            \"max_depth\": 16,\n",
    "            \"num_leaves\": 500,\n",
    "            \"min_data_in_leaf\": 50,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        self.lgb_params_consumption = {\n",
    "            \"n_estimators\": 3000,\n",
    "            \"learning_rate\": 0.072314526,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            \"lambda_l1\": 3.5,\n",
    "            \"lambda_l2\": 1.5,\n",
    "            \"max_depth\": 24,\n",
    "            \"num_leaves\": 500,\n",
    "            \"min_data_in_leaf\": 50,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        \n",
    "        self.nn_model_consumption = DeepTable(config=self.conf)  \n",
    "        self.nn_model_production = DeepTable(config=self.conf)\n",
    "        \n",
    "        self.lgb_model_consumption = lgb.LGBMRegressor(**self.lgb_params_consumption)\n",
    "        self.lgb_model_production = lgb.LGBMRegressor(**self.lgb_params_production)\n",
    "        \n",
    "        #self.lgb_model_consumption = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"consumption_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_consumption, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        #self.lgb_model_production = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"production_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_production, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        \n",
    "    def fit(self, df_train_features):\n",
    "        print('nn = '+str(CFG.nn))\n",
    "        print('lgb = '+str(CFG.lgb))\n",
    "        \n",
    "        if CFG.nn == True:\n",
    "            \n",
    "            print('\\n',\"nn model consumption training.\",'\\n')\n",
    "            mask = df_train_features[\"is_consumption\"] == 1\n",
    "            self.nn_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "            # Avoid saving error\n",
    "            with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "                for i, var in enumerate(CFG.optimizer.weights):\n",
    "                    name = 'variable{}'.format(i)\n",
    "                    CFG.optimizer.weights[i] = tf.Variable(var, name=name)\n",
    "            self.conf = self.conf._replace(optimizer=CFG.optimizer)   \n",
    "            self.nn_model_production = DeepTable(config=self.conf)\n",
    "            \n",
    "            print('\\n',\"nn model production training.\",'\\n')\n",
    "            mask = df_train_features[\"is_consumption\"] == 0\n",
    "            self.nn_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "        if CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption training.\")\n",
    "            mask = df_train_features[\"is_consumption\"] == 1\n",
    "            self.lgb_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"lgb model production training.\",'\\n')\n",
    "            mask = df_train_features[\"is_consumption\"] == 0\n",
    "            self.lgb_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "    def plot_nn_model(self):\n",
    "        if CFG.nn == True:\n",
    "            return plot_model(self.nn_model_consumption.get_model().model)    \n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "        \n",
    "        if CFG.nn == True and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"nn & lgb model consumption prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 1\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_consumption.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_consumption.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"nn & lgb model production prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 0\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_production.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_production.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "        elif CFG.nn == True and CFG.lgb == False:\n",
    "            \n",
    "            print('\\n',\"nn model consumption prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 1\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_consumption.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"nn model production prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 0\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_production.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        elif CFG.nn == False and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 1\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_consumption.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"lgb model production prediction.\",'\\n')\n",
    "            mask = df_features[\"is_consumption\"] == 0\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_production.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No models has been trained.\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "model = Model()\n",
    "model.fit(df_train_weather)\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4dd7967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T02:38:46.363156Z",
     "iopub.status.busy": "2024-01-30T02:38:46.362608Z",
     "iopub.status.idle": "2024-01-30T03:17:58.616690Z",
     "shell.execute_reply": "2024-01-30T03:17:58.615246Z"
    },
    "papermill": {
     "duration": 2352.329048,
     "end_time": "2024-01-30T03:17:58.657255",
     "exception": false,
     "start_time": "2024-01-30T02:38:46.328207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn = True\n",
      "lgb = True\n",
      "\n",
      " nn model consumption training. \n",
      "\n",
      "01-30 02:38:47 I deeptables.m.deeptable.py 338 - X.Shape=(378028, 218), y.Shape=(378028,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e3d9183fe80>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 02:38:47 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 02:38:48 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 02:38:49 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 02:38:50 I deeptables.m.preprocessor.py 336 - Preparing features taken 1.5175926685333252s\n",
      "01-30 02:38:50 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 02:38:55 I deeptables.m.preprocessor.py 383 - Imputation taken 4.967902183532715s\n",
      "01-30 02:38:55 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 02:38:55 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.091522216796875s\n",
      "01-30 02:38:55 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 02:38:55 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 02:39:02 I deeptables.m.preprocessor.py 404 - Discretization taken 6.5470335483551025s\n",
      "01-30 02:39:04 I deeptables.m.preprocessor.py 196 - fit_transform taken 15.41740894317627s\n",
      "01-30 02:39:04 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 02:39:04 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_m, patience:1, mode:min\n",
      "01-30 02:39:05 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 02:39:06 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 02:39:06 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 02:39:08 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (216)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [16, 5, 33, 4, 4, 4, 4, 3, 6, 6, 4, 4, 6, 8, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 21, 13, 21, 13, 21, 13, 21, 12, 25, 27, 26, 27, 27, 27, 22, 22, 13, 12, 27, 27, 27, 6, 8]\n",
      "output_dims: [8, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1484)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1484), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 02:39:08 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "701/701 - 88s - loss: 16.8837 - mean_absolute_error: 16.8837 - val_loss: 14.0059 - val_mean_absolute_error: 14.0059 - 88s/epoch - 126ms/step\n",
      "Epoch 2/10\n",
      "701/701 - 44s - loss: 13.9563 - mean_absolute_error: 13.9563 - val_loss: 12.3943 - val_mean_absolute_error: 12.3943 - 44s/epoch - 63ms/step\n",
      "Epoch 3/10\n",
      "701/701 - 44s - loss: 13.0115 - mean_absolute_error: 13.0115 - val_loss: 11.5686 - val_mean_absolute_error: 11.5686 - 44s/epoch - 63ms/step\n",
      "Epoch 4/10\n",
      "701/701 - 44s - loss: 12.3748 - mean_absolute_error: 12.3748 - val_loss: 10.9016 - val_mean_absolute_error: 10.9016 - 44s/epoch - 63ms/step\n",
      "Epoch 5/10\n",
      "701/701 - 45s - loss: 11.9063 - mean_absolute_error: 11.9063 - val_loss: 10.4837 - val_mean_absolute_error: 10.4837 - 45s/epoch - 64ms/step\n",
      "Epoch 6/10\n",
      "701/701 - 44s - loss: 11.5160 - mean_absolute_error: 11.5160 - val_loss: 10.3691 - val_mean_absolute_error: 10.3691 - 44s/epoch - 63ms/step\n",
      "Epoch 7/10\n",
      "701/701 - 45s - loss: 11.1788 - mean_absolute_error: 11.1788 - val_loss: 9.8102 - val_mean_absolute_error: 9.8102 - 45s/epoch - 64ms/step\n",
      "Epoch 8/10\n",
      "701/701 - 44s - loss: 10.9093 - mean_absolute_error: 10.9093 - val_loss: 10.0786 - val_mean_absolute_error: 10.0786 - 44s/epoch - 63ms/step\n",
      "Epoch 9/10\n",
      "701/701 - 45s - loss: 10.6128 - mean_absolute_error: 10.6128 - val_loss: 9.7861 - val_mean_absolute_error: 9.7861 - 45s/epoch - 64ms/step\n",
      "Epoch 10/10\n",
      "701/701 - 47s - loss: 10.4378 - mean_absolute_error: 10.4378 - val_loss: 9.4534 - val_mean_absolute_error: 9.4534 - 47s/epoch - 67ms/step\n",
      "01-30 02:47:19 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 02:47:19 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 02:47:20 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130023846_dnn_nets/dnn_nets.h5\n",
      "\n",
      " nn model production training. \n",
      "\n",
      "01-30 02:47:21 I deeptables.m.deeptable.py 338 - X.Shape=(378028, 218), y.Shape=(378028,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e3d9183fe80>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 02:47:21 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 02:47:22 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 02:47:22 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 02:47:23 I deeptables.m.preprocessor.py 336 - Preparing features taken 1.225921869277954s\n",
      "01-30 02:47:23 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 02:47:27 I deeptables.m.preprocessor.py 383 - Imputation taken 3.8120369911193848s\n",
      "01-30 02:47:27 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 02:47:27 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.08145380020141602s\n",
      "01-30 02:47:27 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 02:47:27 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 02:47:33 I deeptables.m.preprocessor.py 404 - Discretization taken 6.066195249557495s\n",
      "01-30 02:47:36 I deeptables.m.preprocessor.py 196 - fit_transform taken 13.65820598602295s\n",
      "01-30 02:47:36 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 02:47:37 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 02:47:39 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 02:47:39 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 02:47:41 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (216)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [16, 5, 33, 4, 4, 4, 4, 3, 6, 6, 4, 4, 6, 8, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 22, 22, 20, 20, 20, 20, 22, 22, 19, 20, 16, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 22, 22, 22, 22, 22, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 12, 19, 12, 18, 12, 18, 12, 22, 25, 24, 24, 24, 24, 21, 21, 13, 12, 24, 24, 24, 6, 8]\n",
      "output_dims: [8, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1484)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1484), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 02:47:41 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "701/701 - 88s - loss: 32.4491 - mean_absolute_error: 32.4491 - val_loss: 27.3334 - val_mean_absolute_error: 27.3334 - 88s/epoch - 125ms/step\n",
      "Epoch 2/10\n",
      "701/701 - 46s - loss: 26.7959 - mean_absolute_error: 26.7959 - val_loss: 23.9757 - val_mean_absolute_error: 23.9757 - 46s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "701/701 - 45s - loss: 25.0588 - mean_absolute_error: 25.0588 - val_loss: 23.1939 - val_mean_absolute_error: 23.1939 - 45s/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "701/701 - 45s - loss: 23.8539 - mean_absolute_error: 23.8539 - val_loss: 21.2773 - val_mean_absolute_error: 21.2773 - 45s/epoch - 65ms/step\n",
      "Epoch 5/10\n",
      "701/701 - 45s - loss: 22.8073 - mean_absolute_error: 22.8073 - val_loss: 20.8731 - val_mean_absolute_error: 20.8731 - 45s/epoch - 65ms/step\n",
      "Epoch 6/10\n",
      "701/701 - 45s - loss: 22.0058 - mean_absolute_error: 22.0058 - val_loss: 20.3228 - val_mean_absolute_error: 20.3228 - 45s/epoch - 64ms/step\n",
      "Epoch 7/10\n",
      "701/701 - 45s - loss: 21.3499 - mean_absolute_error: 21.3499 - val_loss: 19.6880 - val_mean_absolute_error: 19.6880 - 45s/epoch - 65ms/step\n",
      "Epoch 8/10\n",
      "701/701 - 45s - loss: 20.5548 - mean_absolute_error: 20.5548 - val_loss: 18.9107 - val_mean_absolute_error: 18.9107 - 45s/epoch - 64ms/step\n",
      "Epoch 9/10\n",
      "701/701 - 45s - loss: 20.0364 - mean_absolute_error: 20.0364 - val_loss: 18.9518 - val_mean_absolute_error: 18.9518 - 45s/epoch - 64ms/step\n",
      "Epoch 10/10\n",
      "701/701 - 47s - loss: 19.4121 - mean_absolute_error: 19.4121 - val_loss: 19.0544 - val_mean_absolute_error: 19.0544 - 47s/epoch - 67ms/step\n",
      "01-30 02:55:57 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 02:55:57 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 02:55:58 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130024720_dnn_nets/dnn_nets.h5\n",
      "\n",
      " lgb model consumption training.\n",
      "\n",
      " lgb model production training. \n",
      "\n",
      "CPU times: user 1h 53min 45s, sys: 2min 27s, total: 1h 56min 13s\n",
      "Wall time: 39min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class CFG:\n",
    "    nn = True\n",
    "    lgb = True\n",
    "    ens_weights = {'nn': 0.5, 'lgb': 0.5}\n",
    "    epochs = 10\n",
    "    batch_size = 512\n",
    "    valid_size = 5e-2\n",
    "    LR_Scheduler = []  # [LR]\n",
    "    optimizer = AdamW(learning_rate=1e-3, weight_decay=9e-7)\n",
    "     \n",
    "class Model_0:\n",
    "    def __init__(self):\n",
    "        self.conf = ModelConfig(auto_imputation=True,\n",
    "                                auto_discrete=True,\n",
    "                                auto_discard_unique=True,\n",
    "                                categorical_columns='auto',\n",
    "                                fixed_embedding_dim=False,\n",
    "                                embeddings_output_dim=4,\n",
    "                                embedding_dropout=0.3,\n",
    "                                nets=['dnn_nets'],\n",
    "                                dnn_params={\n",
    "                                    'hidden_units': ((512, 0.3, True),\n",
    "                                                     (256, 0.3, True)),\n",
    "                                    'dnn_activation': 'relu',\n",
    "                                },\n",
    "                                stacking_op='add',\n",
    "                                output_use_bias=False,\n",
    "                                optimizer=CFG.optimizer,\n",
    "                                task='regression',\n",
    "                                loss='MeanAbsoluteError',\n",
    "                                metrics='MeanAbsoluteError',\n",
    "                                earlystopping_patience=1,\n",
    "                                )\n",
    "        \n",
    "        self.lgb_params_production = {\n",
    "            \"n_estimators\": 4816,\n",
    "            \"learning_rate\": 0.05762004005019972,\n",
    "            \"colsample_bytree\": 0.5329267289068419,\n",
    "            \"colsample_bynode\": 0.8190524860421544,\n",
    "            \"lambda_l1\": 6.826342308358434,\n",
    "            \"lambda_l2\": 9.896390141984359,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_data_in_leaf\": 12,\n",
    "            'max_bin': 836,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        \n",
    "        self.lgb_params_consumption = {\n",
    "            \"n_estimators\": 4468,\n",
    "            \"learning_rate\": 0.051587155894154094,\n",
    "            \"colsample_bytree\": 0.5744556598680428,\n",
    "            \"colsample_bynode\": 0.8156506256005505,\n",
    "            \"lambda_l1\": 1.6751103987805591,\n",
    "            \"lambda_l2\": 5.13700667827768,\n",
    "            \"max_depth\": 9,\n",
    "            \"min_data_in_leaf\": 173,\n",
    "            'max_bin': 151,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.nn_model_consumption = DeepTable(config=self.conf)  \n",
    "        self.nn_model_production = DeepTable(config=self.conf)\n",
    "        \n",
    "        self.lgb_model_consumption = lgb.LGBMRegressor(**self.lgb_params_consumption)\n",
    "        self.lgb_model_production = lgb.LGBMRegressor(**self.lgb_params_production)\n",
    "        \n",
    "        #self.lgb_model_consumption = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"consumption_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_consumption, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        #self.lgb_model_production = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"production_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_production, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        \n",
    "    def fit(self, df_train_features):\n",
    "        print('nn = '+str(CFG.nn))\n",
    "        print('lgb = '+str(CFG.lgb))\n",
    "        \n",
    "        if CFG.nn == True:\n",
    "            \n",
    "            print('\\n',\"nn model consumption training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 0) & (df_train_features['is_consumption'] == 1)\n",
    "            self.nn_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "            # Avoid saving error\n",
    "            with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "                for i, var in enumerate(CFG.optimizer.weights):\n",
    "                    name = 'variable{}'.format(i)\n",
    "                    CFG.optimizer.weights[i] = tf.Variable(var, name=name)\n",
    "            self.conf = self.conf._replace(optimizer=CFG.optimizer)   \n",
    "            self.nn_model_production = DeepTable(config=self.conf)\n",
    "            \n",
    "            print('\\n',\"nn model production training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 0) & (df_train_features['is_consumption'] == 0)\n",
    "            self.nn_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "        if CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption training.\")\n",
    "            mask = (df_train_features[\"is_business\"] == 0) & (df_train_features['is_consumption'] == 1)\n",
    "            self.lgb_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"lgb model production training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 0) & (df_train_features['is_consumption'] == 0)\n",
    "            self.lgb_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "    def plot_nn_model(self):\n",
    "        if CFG.nn == True:\n",
    "            return plot_model(self.nn_model_consumption.get_model().model)    \n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "        \n",
    "        if CFG.nn == True and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"nn & lgb model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_consumption.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_consumption.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"nn & lgb model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_production.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_production.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "        elif CFG.nn == True and CFG.lgb == False:\n",
    "            \n",
    "            print('\\n',\"nn model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_consumption.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"nn model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_production.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        elif CFG.nn == False and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_consumption.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"lgb model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 0) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_production.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No models has been trained.\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "model_0 = Model_0()\n",
    "model_0.fit(df_train_weather)\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052f2222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T03:17:58.735517Z",
     "iopub.status.busy": "2024-01-30T03:17:58.734738Z",
     "iopub.status.idle": "2024-01-30T04:03:39.471483Z",
     "shell.execute_reply": "2024-01-30T04:03:39.470381Z"
    },
    "papermill": {
     "duration": 2740.830194,
     "end_time": "2024-01-30T04:03:39.524306",
     "exception": false,
     "start_time": "2024-01-30T03:17:58.694112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn = True\n",
      "lgb = True\n",
      "\n",
      " nn model consumption training. \n",
      "\n",
      "01-30 03:17:59 I deeptables.m.deeptable.py 338 - X.Shape=(447923, 218), y.Shape=(447923,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e40de3f02b0>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 03:17:59 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 03:18:00 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 03:18:01 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 03:18:02 I deeptables.m.preprocessor.py 336 - Preparing features taken 1.6579830646514893s\n",
      "01-30 03:18:02 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 03:18:07 I deeptables.m.preprocessor.py 383 - Imputation taken 4.593993186950684s\n",
      "01-30 03:18:07 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 03:18:07 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.10190010070800781s\n",
      "01-30 03:18:07 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 03:18:07 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 03:18:15 I deeptables.m.preprocessor.py 404 - Discretization taken 7.5214338302612305s\n",
      "01-30 03:18:17 I deeptables.m.preprocessor.py 196 - fit_transform taken 16.891738176345825s\n",
      "01-30 03:18:17 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 03:18:17 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_m, patience:1, mode:min\n",
      "01-30 03:18:19 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 03:18:22 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 03:18:23 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 03:18:24 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (216)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [18, 6, 40, 4, 4, 4, 4, 3, 6, 6, 4, 4, 6, 7, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 17, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 23, 23, 23, 23, 23, 23, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 23, 13, 23, 13, 23, 13, 23, 12, 27, 28, 27, 28, 28, 28, 23, 23, 13, 12, 28, 28, 28, 6, 7]\n",
      "output_dims: [8, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1484)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1484), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 03:18:24 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "831/831 - 95s - loss: 146.1320 - mean_absolute_error: 146.1320 - val_loss: 106.8119 - val_mean_absolute_error: 106.8119 - 95s/epoch - 115ms/step\n",
      "Epoch 2/10\n",
      "831/831 - 54s - loss: 98.0070 - mean_absolute_error: 98.0070 - val_loss: 84.6471 - val_mean_absolute_error: 84.6471 - 54s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "831/831 - 54s - loss: 87.5202 - mean_absolute_error: 87.5202 - val_loss: 76.9187 - val_mean_absolute_error: 76.9187 - 54s/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "831/831 - 54s - loss: 83.2311 - mean_absolute_error: 83.2311 - val_loss: 72.5098 - val_mean_absolute_error: 72.5098 - 54s/epoch - 64ms/step\n",
      "Epoch 5/10\n",
      "831/831 - 53s - loss: 79.9454 - mean_absolute_error: 79.9454 - val_loss: 69.9262 - val_mean_absolute_error: 69.9262 - 53s/epoch - 64ms/step\n",
      "Epoch 6/10\n",
      "831/831 - 54s - loss: 77.4450 - mean_absolute_error: 77.4450 - val_loss: 70.2524 - val_mean_absolute_error: 70.2524 - 54s/epoch - 65ms/step\n",
      "Epoch 7/10\n",
      "831/831 - 54s - loss: 75.4294 - mean_absolute_error: 75.4294 - val_loss: 67.1497 - val_mean_absolute_error: 67.1497 - 54s/epoch - 65ms/step\n",
      "Epoch 8/10\n",
      "831/831 - 54s - loss: 73.3929 - mean_absolute_error: 73.3929 - val_loss: 65.1968 - val_mean_absolute_error: 65.1968 - 54s/epoch - 65ms/step\n",
      "Epoch 9/10\n",
      "831/831 - 55s - loss: 71.7857 - mean_absolute_error: 71.7857 - val_loss: 63.2014 - val_mean_absolute_error: 63.2014 - 55s/epoch - 66ms/step\n",
      "Epoch 10/10\n",
      "831/831 - 56s - loss: 70.4631 - mean_absolute_error: 70.4631 - val_loss: 61.9227 - val_mean_absolute_error: 61.9227 - 56s/epoch - 67ms/step\n",
      "01-30 03:28:27 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 03:28:27 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 03:28:28 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130031758_dnn_nets/dnn_nets.h5\n",
      "\n",
      " nn model production training. \n",
      "\n",
      "01-30 03:28:29 I deeptables.m.deeptable.py 338 - X.Shape=(447923, 218), y.Shape=(447923,), batch_size=512, config=ModelConfig(name='conf-1', nets=['dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics='MeanAbsoluteError', auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=True, auto_discrete=True, auto_discard_unique=True, apply_gbm_features=False, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=False, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.3, stacking_op='add', output_use_bias=False, apply_class_weight=False, optimizer=<tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7e40de3f02b0>, loss='MeanAbsoluteError', dnn_params={'hidden_units': ((512, 0.3, True), (256, 0.3, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "01-30 03:28:29 I deeptables.m.deeptable.py 339 - metrics:MeanAbsoluteError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01-30 03:28:30 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-30 03:28:30 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "01-30 03:28:32 I deeptables.m.preprocessor.py 336 - Preparing features taken 1.491227149963379s\n",
      "01-30 03:28:32 I deeptables.m.preprocessor.py 341 - Data imputation...\n",
      "01-30 03:28:37 I deeptables.m.preprocessor.py 383 - Imputation taken 4.6638572216033936s\n",
      "01-30 03:28:37 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "01-30 03:28:37 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.0992588996887207s\n",
      "01-30 03:28:37 I deeptables.m.preprocessor.py 398 - Data discretization...\n",
      "01-30 03:28:37 I hypernets.t.sklearn_ex.py 716 - 212 variables to discrete.\n",
      "01-30 03:28:44 I deeptables.m.preprocessor.py 404 - Discretization taken 6.921899318695068s\n",
      "01-30 03:28:46 I deeptables.m.preprocessor.py 196 - fit_transform taken 15.796026468276978s\n",
      "01-30 03:28:46 I deeptables.m.deeptable.py 354 - Training...\n",
      "01-30 03:28:47 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 03:28:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=True\n",
      "01-30 03:28:50 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "01-30 03:28:52 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (216)', 'input_continuous_all: (212)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [18, 6, 40, 4, 4, 4, 4, 3, 6, 6, 4, 4, 6, 7, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 13, 13, 12, 13, 12, 12, 13, 13, 11, 11, 11, 12, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 16, 19, 23, 22, 20, 20, 20, 20, 23, 23, 20, 20, 17, 19, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 12, 12, 7, 7, 12, 11, 11, 11, 11, 12, 12, 11, 11, 11, 11, 10, 6, 6, 12, 8, 8, 8, 8, 10, 10, 12, 11, 10, 10, 10, 7, 6, 11, 10, 10, 10, 10, 10, 10, 9, 9, 9, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 22, 22, 20, 20, 20, 20, 20, 20, 23, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 20, 20, 20, 19, 12, 19, 12, 19, 12, 19, 12, 22, 24, 24, 24, 24, 24, 21, 21, 13, 12, 24, 24, 24, 6, 7]\n",
      "output_dims: [8, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 8, 8, 8, 4, 4]\n",
      "dropout: 0.3\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1484)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 1484), output_shape (None, 256)\n",
      "---------------------------------------------------------\n",
      "stacking_op: add\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: MeanAbsoluteError\n",
      "optimizer: AdamW\n",
      "---------------------------------------------------------\n",
      "\n",
      "01-30 03:28:52 I deeptables.m.deepmodel.py 105 - training...\n",
      "Epoch 1/10\n",
      "831/831 - 96s - loss: 33.3429 - mean_absolute_error: 33.3429 - val_loss: 24.2279 - val_mean_absolute_error: 24.2279 - 96s/epoch - 115ms/step\n",
      "Epoch 2/10\n",
      "831/831 - 54s - loss: 28.2410 - mean_absolute_error: 28.2410 - val_loss: 21.8865 - val_mean_absolute_error: 21.8865 - 54s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "831/831 - 53s - loss: 26.6398 - mean_absolute_error: 26.6398 - val_loss: 20.8750 - val_mean_absolute_error: 20.8750 - 53s/epoch - 64ms/step\n",
      "Epoch 4/10\n",
      "831/831 - 54s - loss: 25.5238 - mean_absolute_error: 25.5238 - val_loss: 19.9720 - val_mean_absolute_error: 19.9720 - 54s/epoch - 65ms/step\n",
      "Epoch 5/10\n",
      "831/831 - 54s - loss: 24.6371 - mean_absolute_error: 24.6371 - val_loss: 19.5331 - val_mean_absolute_error: 19.5331 - 54s/epoch - 65ms/step\n",
      "Epoch 6/10\n",
      "831/831 - 54s - loss: 23.7603 - mean_absolute_error: 23.7603 - val_loss: 18.8561 - val_mean_absolute_error: 18.8561 - 54s/epoch - 65ms/step\n",
      "Epoch 7/10\n",
      "831/831 - 54s - loss: 22.9404 - mean_absolute_error: 22.9404 - val_loss: 18.0931 - val_mean_absolute_error: 18.0931 - 54s/epoch - 65ms/step\n",
      "Epoch 8/10\n",
      "831/831 - 56s - loss: 22.3402 - mean_absolute_error: 22.3402 - val_loss: 18.0131 - val_mean_absolute_error: 18.0131 - 56s/epoch - 67ms/step\n",
      "Epoch 9/10\n",
      "831/831 - 54s - loss: 21.7837 - mean_absolute_error: 21.7837 - val_loss: 17.4961 - val_mean_absolute_error: 17.4961 - 54s/epoch - 65ms/step\n",
      "Epoch 10/10\n",
      "831/831 - 55s - loss: 21.2233 - mean_absolute_error: 21.2233 - val_loss: 17.3664 - val_mean_absolute_error: 17.3664 - 55s/epoch - 66ms/step\n",
      "01-30 03:38:35 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "01-30 03:38:36 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "01-30 03:38:36 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20240130032828_dnn_nets/dnn_nets.h5\n",
      "\n",
      " lgb model consumption training.\n",
      "\n",
      " lgb model production training. \n",
      "\n",
      "CPU times: user 2h 15min 39s, sys: 2min 24s, total: 2h 18min 3s\n",
      "Wall time: 45min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class CFG:\n",
    "    nn = True\n",
    "    lgb = True\n",
    "    ens_weights = {'nn': 0.5, 'lgb': 0.5}\n",
    "    epochs = 10\n",
    "    batch_size = 512\n",
    "    valid_size = 5e-2\n",
    "    LR_Scheduler = []  # [LR]\n",
    "    optimizer = AdamW(learning_rate=1e-3, weight_decay=9e-7)\n",
    "     \n",
    "class Model_1:\n",
    "    def __init__(self):\n",
    "        self.conf = ModelConfig(auto_imputation=True,\n",
    "                                auto_discrete=True,\n",
    "                                auto_discard_unique=True,\n",
    "                                categorical_columns='auto',\n",
    "                                fixed_embedding_dim=False,\n",
    "                                embeddings_output_dim=4,\n",
    "                                embedding_dropout=0.3,\n",
    "                                nets=['dnn_nets'],\n",
    "                                dnn_params={\n",
    "                                    'hidden_units': ((512, 0.3, True),\n",
    "                                                     (256, 0.3, True)),\n",
    "                                    'dnn_activation': 'relu',\n",
    "                                },\n",
    "                                stacking_op='add',\n",
    "                                output_use_bias=False,\n",
    "                                optimizer=CFG.optimizer,\n",
    "                                task='regression',\n",
    "                                loss='MeanAbsoluteError',\n",
    "                                metrics='MeanAbsoluteError',\n",
    "                                earlystopping_patience=1,\n",
    "                                )\n",
    "        \n",
    "        self.lgb_params_production = {\n",
    "            \"n_estimators\": 3906,\n",
    "            \"learning_rate\": 0.08313616746400578,\n",
    "            \"colsample_bytree\": 0.9091800966076276,\n",
    "            \"colsample_bynode\": 0.662635011373687,\n",
    "            \"lambda_l1\": 8.315655499927392,\n",
    "            \"lambda_l2\": 8.349555455794054,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_data_in_leaf\": 150,\n",
    "            'max_bin': 819,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        \n",
    "        self.lgb_params_consumption = {\n",
    "            \"n_estimators\": 3183,\n",
    "            \"learning_rate\": 0.03536956293355251,\n",
    "            \"colsample_bytree\": 0.5729074917490897,\n",
    "            \"colsample_bynode\": 0.8596519737421342,\n",
    "            \"lambda_l1\": 7.316399785964528,\n",
    "            \"lambda_l2\": 9.952037457652327,\n",
    "            \"max_depth\": 9,\n",
    "            \"min_data_in_leaf\": 218,\n",
    "            'max_bin': 51,\n",
    "            \"objective\": \"regression_l1\",\n",
    "            \"device\": \"cpu\",\n",
    "        }\n",
    "        \n",
    "        self.nn_model_consumption = DeepTable(config=self.conf)  \n",
    "        self.nn_model_production = DeepTable(config=self.conf)\n",
    "        \n",
    "        self.lgb_model_consumption = lgb.LGBMRegressor(**self.lgb_params_consumption)\n",
    "        self.lgb_model_production = lgb.LGBMRegressor(**self.lgb_params_production)\n",
    "        \n",
    "        #self.lgb_model_consumption = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"consumption_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_consumption, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        #self.lgb_model_production = VotingRegressor(\n",
    "        #    [\n",
    "        #        (\n",
    "        #            f\"production_lgb_{i}\",\n",
    "        #            lgb.LGBMRegressor(**self.lgb_params_production, random_state=i),\n",
    "        #        )\n",
    "        #        for i in range(3)\n",
    "        #    ]\n",
    "        #)\n",
    "        \n",
    "    def fit(self, df_train_features):\n",
    "        print('nn = '+str(CFG.nn))\n",
    "        print('lgb = '+str(CFG.lgb))\n",
    "        \n",
    "        if CFG.nn == True:\n",
    "            \n",
    "            print('\\n',\"nn model consumption training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 1) & (df_train_features['is_consumption'] == 1)\n",
    "            self.nn_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "            # Avoid saving error\n",
    "            with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "                for i, var in enumerate(CFG.optimizer.weights):\n",
    "                    name = 'variable{}'.format(i)\n",
    "                    CFG.optimizer.weights[i] = tf.Variable(var, name=name)\n",
    "            self.conf = self.conf._replace(optimizer=CFG.optimizer)   \n",
    "            self.nn_model_production = DeepTable(config=self.conf)\n",
    "            \n",
    "            print('\\n',\"nn model production training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 1) & (df_train_features['is_consumption'] == 0)\n",
    "            self.nn_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "                validation_split=CFG.valid_size, shuffle=False,\n",
    "                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n",
    "                callbacks=CFG.LR_Scheduler\n",
    "            )\n",
    "        \n",
    "        if CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption training.\")\n",
    "            mask = (df_train_features[\"is_business\"] == 1) & (df_train_features['is_consumption'] == 1)\n",
    "            self.lgb_model_consumption.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"lgb model production training.\",'\\n')\n",
    "            mask = (df_train_features[\"is_business\"] == 1) & (df_train_features['is_consumption'] == 0)\n",
    "            self.lgb_model_production.fit(\n",
    "                X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "                y=df_train_features[mask][\"target\"]\n",
    "                - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "            )\n",
    "        \n",
    "    def plot_nn_model(self):\n",
    "        if CFG.nn == True:\n",
    "            return plot_model(self.nn_model_consumption.get_model().model)    \n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "        \n",
    "        if CFG.nn == True and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"nn & lgb model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_consumption.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_consumption.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "            print('\\n',\"nn & lgb model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + CFG.ens_weights['nn'] * (self.nn_model_production.predict(df_features[mask])[:,0])\n",
    "                + CFG.ens_weights['lgb'] * (self.lgb_model_production.predict(df_features[mask])),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "        \n",
    "        elif CFG.nn == True and CFG.lgb == False:\n",
    "            \n",
    "            print('\\n',\"nn model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_consumption.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"nn model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.nn_model_production.predict(df_features[mask])[:,0],\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        elif CFG.nn == False and CFG.lgb == True:\n",
    "            \n",
    "            print('\\n',\"lgb model consumption prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 1)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_consumption.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "            print('\\n',\"lgb model production prediction.\",'\\n')\n",
    "            mask = (df_features[\"is_business\"] == 1) & (df_features['is_consumption'] == 0)\n",
    "            predictions[mask.values] = np.clip(\n",
    "                df_features[mask][\"target_48h\"].fillna(0).values\n",
    "                + self.lgb_model_production.predict(df_features[mask]),\n",
    "                0,\n",
    "                np.inf,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No models has been trained.\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "model_1 = Model_1()\n",
    "model_1.fit(df_train_weather)\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc60bda",
   "metadata": {
    "papermill": {
     "duration": 0.04501,
     "end_time": "2024-01-30T04:03:39.614374",
     "exception": false,
     "start_time": "2024-01-30T04:03:39.569364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94075ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:03:39.709450Z",
     "iopub.status.busy": "2024-01-30T04:03:39.708767Z",
     "iopub.status.idle": "2024-01-30T04:04:40.084487Z",
     "shell.execute_reply": "2024-01-30T04:04:40.083236Z"
    },
    "papermill": {
     "duration": 60.472146,
     "end_time": "2024-01-30T04:04:40.132100",
     "exception": false,
     "start_time": "2024-01-30T04:03:39.659954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\n",
    "p1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fb0b029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:04:40.225915Z",
     "iopub.status.busy": "2024-01-30T04:04:40.225038Z",
     "iopub.status.idle": "2024-01-30T04:05:49.843199Z",
     "shell.execute_reply": "2024-01-30T04:05:49.842036Z"
    },
    "papermill": {
     "duration": 69.711837,
     "end_time": "2024-01-30T04:05:49.890009",
     "exception": false,
     "start_time": "2024-01-30T04:04:40.178172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\n",
    "dp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adbc48",
   "metadata": {
    "papermill": {
     "duration": 0.045609,
     "end_time": "2024-01-30T04:05:49.980986",
     "exception": false,
     "start_time": "2024-01-30T04:05:49.935377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Declaring separate prediction functions for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "788d3f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:05:50.074426Z",
     "iopub.status.busy": "2024-01-30T04:05:50.073979Z",
     "iopub.status.idle": "2024-01-30T04:05:50.081997Z",
     "shell.execute_reply": "2024-01-30T04:05:50.080591Z"
    },
    "papermill": {
     "duration": 0.057905,
     "end_time": "2024-01-30T04:05:50.084504",
     "exception": false,
     "start_time": "2024-01-30T04:05:50.026599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(df_features,model_consumption=c1,model_production=p1):\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 1\n",
    "    predictions[mask.values] = model_consumption.predict(\n",
    "            df_features[mask]\n",
    "    ).clip(0)\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 0\n",
    "    predictions[mask.values] = model_production.predict(\n",
    "            df_features[mask]\n",
    "    ).clip(0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed879ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:05:50.179638Z",
     "iopub.status.busy": "2024-01-30T04:05:50.179175Z",
     "iopub.status.idle": "2024-01-30T04:05:50.187009Z",
     "shell.execute_reply": "2024-01-30T04:05:50.186169Z"
    },
    "papermill": {
     "duration": 0.057612,
     "end_time": "2024-01-30T04:05:50.189253",
     "exception": false,
     "start_time": "2024-01-30T04:05:50.131641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 1\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_consumption.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 0\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_production.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02f8efb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:05:50.283300Z",
     "iopub.status.busy": "2024-01-30T04:05:50.282578Z",
     "iopub.status.idle": "2024-01-30T04:05:50.315617Z",
     "shell.execute_reply": "2024-01-30T04:05:50.314724Z"
    },
    "papermill": {
     "duration": 0.08245,
     "end_time": "2024-01-30T04:05:50.318326",
     "exception": false,
     "start_time": "2024-01-30T04:05:50.235876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import enefit\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc2e789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T04:05:50.412208Z",
     "iopub.status.busy": "2024-01-30T04:05:50.411454Z",
     "iopub.status.idle": "2024-01-30T04:09:53.368057Z",
     "shell.execute_reply": "2024-01-30T04:09:53.366861Z"
    },
    "papermill": {
     "duration": 243.007377,
     "end_time": "2024-01-30T04:09:53.371375",
     "exception": false,
     "start_time": "2024-01-30T04:05:50.363998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:06:38 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:38 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:38 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5126051902770996s\n",
      "01-30 04:06:38 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:38 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:40 I deeptables.m.deeptable.py 559 - predict_proba taken 2.0409951210021973s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:06:41 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:41 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:41 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5094857215881348s\n",
      "01-30 04:06:41 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:41 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:43 I deeptables.m.deeptable.py 559 - predict_proba taken 1.9387423992156982s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:06:43 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:43 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:44 I deeptables.m.preprocessor.py 249 - transform_X taken 0.6608729362487793s\n",
      "01-30 04:06:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:45 I deeptables.m.deeptable.py 559 - predict_proba taken 2.015155076980591s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:06:46 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:46 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:46 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5055582523345947s\n",
      "01-30 04:06:46 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:46 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:47 I deeptables.m.deeptable.py 559 - predict_proba taken 1.8334846496582031s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:06:48 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:48 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:48 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5128545761108398s\n",
      "01-30 04:06:48 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:48 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:50 I deeptables.m.deeptable.py 559 - predict_proba taken 1.909355640411377s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:06:50 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:06:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:06:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5006678104400635s\n",
      "01-30 04:06:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:06:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:06:52 I deeptables.m.deeptable.py 559 - predict_proba taken 1.8328561782836914s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:07:39 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:39 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5313618183135986s\n",
      "01-30 04:07:40 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:40 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6915874481201172s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:07:41 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:41 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:41 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5169751644134521s\n",
      "01-30 04:07:41 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:41 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:42 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6746830940246582s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:07:42 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:42 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:43 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5062940120697021s\n",
      "01-30 04:07:43 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:43 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6198997497558594s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:07:43 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:43 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:43 I deeptables.m.preprocessor.py 249 - transform_X taken 0.49555206298828125s\n",
      "01-30 04:07:43 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:44 I deeptables.m.deeptable.py 559 - predict_proba taken 0.601447343826294s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:07:44 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:44 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:44 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5044183731079102s\n",
      "01-30 04:07:44 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:44 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:44 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6186344623565674s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:07:45 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:07:45 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:07:45 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5128283500671387s\n",
      "01-30 04:07:45 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:07:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:07:45 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6189625263214111s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:08:50 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.521841287612915s\n",
      "01-30 04:08:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:50 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6862621307373047s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:08:51 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:51 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:51 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5095980167388916s\n",
      "01-30 04:08:51 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:52 I deeptables.m.deeptable.py 559 - predict_proba taken 0.679863452911377s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:08:52 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:52 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:53 I deeptables.m.preprocessor.py 249 - transform_X taken 0.540367841720581s\n",
      "01-30 04:08:53 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:53 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:53 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6562519073486328s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:08:53 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:53 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:54 I deeptables.m.preprocessor.py 249 - transform_X taken 0.558427095413208s\n",
      "01-30 04:08:54 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:54 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:54 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6674458980560303s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:08:54 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:54 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:55 I deeptables.m.preprocessor.py 249 - transform_X taken 0.4938080310821533s\n",
      "01-30 04:08:55 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:55 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:55 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6068277359008789s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:08:55 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:08:55 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:08:55 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5014102458953857s\n",
      "01-30 04:08:55 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:08:55 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:08:55 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6043200492858887s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:09:45 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:45 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:45 I deeptables.m.preprocessor.py 249 - transform_X taken 0.508528470993042s\n",
      "01-30 04:09:45 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:45 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:45 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6738801002502441s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:09:46 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:46 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:47 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5118899345397949s\n",
      "01-30 04:09:47 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:47 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:47 I deeptables.m.deeptable.py 559 - predict_proba taken 0.7010824680328369s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:09:47 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:47 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:48 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5044283866882324s\n",
      "01-30 04:09:48 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:48 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:48 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6068980693817139s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:09:48 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:48 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:49 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5302083492279053s\n",
      "01-30 04:09:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:49 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:49 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6380875110626221s\n",
      "\n",
      " nn & lgb model consumption prediction. \n",
      "\n",
      "01-30 04:09:49 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:49 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.506103515625s\n",
      "01-30 04:09:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:50 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6938183307647705s\n",
      "\n",
      " nn & lgb model production prediction. \n",
      "\n",
      "01-30 04:09:50 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "01-30 04:09:50 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "01-30 04:09:50 I deeptables.m.preprocessor.py 249 - transform_X taken 0.5037832260131836s\n",
      "01-30 04:09:50 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "01-30 04:09:50 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=False, drop_remainder=False\n",
      "01-30 04:09:50 I deeptables.m.deeptable.py 559 - predict_proba taken 0.6132252216339111s\n"
     ]
    }
   ],
   "source": [
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "\n",
    "    data_storage.update_with_new_data(\n",
    "        df_new_client=df_new_client,\n",
    "        df_new_gas_prices=df_new_gas_prices,\n",
    "        df_new_electricity_prices=df_new_electricity_prices,\n",
    "        df_new_forecast_weather=df_new_forecast_weather,\n",
    "        df_new_historical_weather=df_new_historical_weather,\n",
    "        df_new_target=df_new_target\n",
    "    )\n",
    "    \n",
    "    df_test = data_storage.preprocess_test(df_test)\n",
    "    \n",
    "    df_test_features = features_generator.generate_features(df_test)\n",
    "    \n",
    "    df_test_feats = feat_gen.generate_features(df_test,False)\n",
    "    \n",
    "    df_test_feats.drop(columns=['date','literal'],inplace=True)\n",
    "    \n",
    "    df_test_weather = feat_weather.generate_features(df_test)\n",
    "    \n",
    "    df_test_weather['installed_capacity'] = df_test_weather['installed_capacity'].fillna(1000)\n",
    "    \n",
    "    pred1 = predict(df_test_features) # lgbm(votingregression) Version1\n",
    "    \n",
    "    pred2 = predict_model(df_test_feats) # lgbm(votingregression) Version2 \n",
    "    \n",
    "    pred3 = model.predict(df_test_weather)\n",
    "      \n",
    "    pred_0 = model_0.predict(df_test_weather)\n",
    "    \n",
    "    pred_1 = model_1.predict(df_test_weather)\n",
    "    \n",
    "    pred4 = (pred_0 + pred_1)\n",
    " \n",
    "    # Ensembling with slightly tuned model weights\n",
    "    df_sample_prediction[\"target\"] = (\n",
    "        (0.25 * pred1) + \n",
    "        (0.3 * pred2) +\n",
    "        (0.3 * pred3) +\n",
    "        (0.15 * pred4)\n",
    "    )\n",
    "    \n",
    "    env.predict(df_sample_prediction)\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 4266997,
     "sourceId": 7348254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4307493,
     "sourceId": 7406639,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 150384981,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 160761304,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14164.140363,
   "end_time": "2024-01-30T04:09:56.813633",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-30T00:13:52.673270",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
